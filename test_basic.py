#!/usr/bin/env python3
"""
Basic test script for Turbo Inference Server
Tests core components without requiring actual model loading
"""

import asyncio
import sys
import time
from typing import Dict, Any

# Add the current directory to Python path
sys.path.insert(0, '.')

from turbo_inference.core.config import EngineConfig, ServerConfig
from turbo_inference.core.engine import TurboEngine
from turbo_inference.core.scheduler import Scheduler
from turbo_inference.core.memory_manager import MemoryManager
from turbo_inference.core.cache_manager import CacheManager
from turbo_inference.models.registry import ModelRegistry


class MockRequest:
    """Mock request for testing"""
    def __init__(self, prompt: str, max_tokens: int = 100, priority: int = 1):
        self.prompt = prompt
        self.max_tokens = max_tokens
        self.priority = priority
        self.id = f"req_{int(time.time() * 1000)}"
        self.created_at = time.time()


async def test_config():
    """Test configuration system"""
    print("üß™ Testing Configuration System...")
    
    # Test EngineConfig
    engine_config = EngineConfig(
        max_model_len=4096,
        max_num_batched_tokens=8192,
        max_num_seqs=256,
        gpu_memory_utilization=0.9,
        enable_cuda_graphs=True
    )
    print(f"‚úÖ EngineConfig created: max_model_len={engine_config.max_model_len}")
    
    # Test ServerConfig
    server_config = ServerConfig(
        host="0.0.0.0",
        port=8000,
        max_concurrent_requests=100,
        request_timeout=30.0
    )
    print(f"‚úÖ ServerConfig created: host={server_config.host}, port={server_config.port}")
    
    return engine_config, server_config


async def test_scheduler():
    """Test scheduler functionality"""
    print("\nüß™ Testing Scheduler...")
    
    scheduler = Scheduler(max_num_seqs=10, max_num_batched_tokens=2048)
    
    # Create mock requests
    requests = [
        MockRequest("Hello world", max_tokens=50, priority=1),
        MockRequest("How are you?", max_tokens=30, priority=2),
        MockRequest("Test prompt", max_tokens=100, priority=1)
    ]
    
    # Add requests to scheduler
    for req in requests:
        scheduler.add_request(req)
    
    print(f"‚úÖ Added {len(requests)} requests to scheduler")
    
    # Test scheduling
    scheduled_requests = scheduler.schedule()
    print(f"‚úÖ Scheduled {len(scheduled_requests)} requests")
    
    return scheduler


async def test_memory_manager():
    """Test memory manager"""
    print("\nüß™ Testing Memory Manager...")
    
    memory_manager = MemoryManager(
        max_model_len=4096,
        max_num_seqs=256,
        gpu_memory_utilization=0.9
    )
    
    # Test memory allocation simulation
    block_size = 512
    num_blocks = 10
    
    blocks = []
    for i in range(num_blocks):
        block = memory_manager.allocate_block(block_size)
        if block:
            blocks.append(block)
    
    print(f"‚úÖ Allocated {len(blocks)} memory blocks")
    
    # Test memory deallocation
    for block in blocks[:5]:
        memory_manager.free_block(block)
    
    print(f"‚úÖ Freed {len(blocks[:5])} memory blocks")
    
    return memory_manager


async def test_cache_manager():
    """Test cache manager"""
    print("\nüß™ Testing Cache Manager...")
    
    cache_manager = CacheManager(
        max_cache_size=1024 * 1024 * 100,  # 100MB
        max_num_entries=1000
    )
    
    # Test caching
    test_key = "test_prompt_123"
    test_response = {"tokens": ["Hello", "world"], "logprobs": [0.1, 0.2]}
    
    cache_manager.set(test_key, test_response)
    print(f"‚úÖ Cached response for key: {test_key}")
    
    # Test retrieval
    cached_response = cache_manager.get(test_key)
    if cached_response:
        print(f"‚úÖ Retrieved cached response: {len(cached_response.get('tokens', []))} tokens")
    else:
        print("‚ùå Failed to retrieve cached response")
    
    return cache_manager


async def test_model_registry():
    """Test model registry"""
    print("\nüß™ Testing Model Registry...")
    
    registry = ModelRegistry()
    
    # Test registration
    from turbo_inference.models.transformers import TransformersModel
    
    registry.register("transformers", TransformersModel)
    print("‚úÖ Registered TransformersModel")
    
    # Test retrieval
    model_class = registry.get("transformers")
    if model_class:
        print(f"‚úÖ Retrieved model class: {model_class.__name__}")
    else:
        print("‚ùå Failed to retrieve model class")
    
    # Test listing
    available_models = registry.list_models()
    print(f"‚úÖ Available models: {available_models}")
    
    return registry


async def test_engine_integration():
    """Test engine integration"""
    print("\nüß™ Testing Engine Integration...")
    
    engine_config = EngineConfig(
        max_model_len=2048,
        max_num_batched_tokens=4096,
        max_num_seqs=50,
        gpu_memory_utilization=0.8
    )
    
    try:
        engine = TurboEngine(engine_config)
        print("‚úÖ TurboEngine created successfully")
        
        # Test request handling (without actual model)
        mock_request = MockRequest("Test prompt", max_tokens=50)
        
        # This would normally require a loaded model, so we'll just test the structure
        print("‚úÖ Engine structure validated")
        
        return engine
    except Exception as e:
        print(f"‚ö†Ô∏è  Engine creation failed (expected without model): {e}")
        return None


async def run_performance_test():
    """Run basic performance test"""
    print("\nüß™ Running Performance Test...")
    
    # Test scheduler performance
    scheduler = Scheduler(max_num_seqs=1000, max_num_batched_tokens=8192)
    
    start_time = time.time()
    
    # Add many requests
    for i in range(1000):
        req = MockRequest(f"Prompt {i}", max_tokens=50, priority=i % 3 + 1)
        scheduler.add_request(req)
    
    add_time = time.time() - start_time
    print(f"‚úÖ Added 1000 requests in {add_time:.4f}s")
    
    # Test scheduling performance
    start_time = time.time()
    for _ in range(100):
        scheduled = scheduler.schedule()
    
    schedule_time = time.time() - start_time
    print(f"‚úÖ Scheduled 100 batches in {schedule_time:.4f}s")
    
    return {
        "add_requests_per_second": 1000 / add_time,
        "schedule_batches_per_second": 100 / schedule_time
    }


async def main():
    """Run all tests"""
    print("üöÄ Starting Turbo Inference Server Tests\n")
    
    try:
        # Test individual components
        await test_config()
        await test_scheduler()
        await test_memory_manager()
        await test_cache_manager()
        await test_model_registry()
        await test_engine_integration()
        
        # Run performance test
        performance_results = await run_performance_test()
        
        print("\nüìä Performance Results:")
        print(f"   Request Addition: {performance_results['add_requests_per_second']:.0f} req/s")
        print(f"   Batch Scheduling: {performance_results['schedule_batches_per_second']:.0f} batches/s")
        
        print("\n‚úÖ All basic tests completed successfully!")
        print("\nüìù Next Steps:")
        print("   1. Install actual model dependencies")
        print("   2. Load a real model for end-to-end testing")
        print("   3. Run comparison benchmarks with vLLM and Triton")
        
    except Exception as e:
        print(f"\n‚ùå Test failed: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    asyncio.run(main()) 